\documentclass[12pt]{article}
\usepackage{fullpage}
\usepackage{amsmath}

\begin{document}

\title{Rudimentary Naive Bayes Classifier In Haskell}
\author{Dhruv Rajan}
\date{\today}
\maketitle

\section{Implementation}
\begin{itemize}
\item The implementation had two main parts: transformations on data representations, and probabilistic inference
\item \textbf{Data Transformations}
  \begin{itemize}
  \item Involved developing ``summary'' statistics representations
  \item Input: a list of $n$-vectors \texttt{[[Float]]}
  \item Final representation: summary statistics of features $(\mu, \sigma)$ per-feature,
    per-class \texttt{[(Int, [(Float, Float)])]}
  \item Creating these ``summary'' statistics comprises ``training'' the model
  \item This could be done much more cleanly, in an ``update'' manner for individual entries,
    as opposed the current pipeline of inefficient transformations, and with more emphasis on training
  \item Additionally, can explore exploiting datatypes and laziness
  \end{itemize}
\item \textbf{Probabilistic Inference}
  \begin{itemize}
  \item Predict classification of new vectors using the Naive Bayes Algorithm.
  \item Must calculate conditional probabilities for every feature - would be better to utilize
    the Distribution abstraction
  \item Available libraries seem to only support manipulation of discrete distributions, and do not
    allow the representation of continuous distributions (gaussian, exponential, etc.)
  \item Areas to look at: continuous distribution abstraction support, utilization of distribution abstraction,
    other inference algorithms, modular sub-algorithms or standard distribution transformations that may
    be common between other machine learning classification algorithms
  \end{itemize}
\end{itemize}

\section{Naive Bayes Algorithm}
Naive Bayes is a classification algorithm which utilizes Baye's Rule:
\begin{align}
  P(A | B) = \frac{P(B | A) \cdot P(A)}{P(B)}
\end{align}

Each entry is a vector, $\langle 1, 3, 7, ... \rangle$, classified by
an integer category $0, 1, 2, 3 ... $. The classifier is a function
which maps a vector to a category: \textit{clf} $:: \chi \rightarrow C$ .
The insight is that learning this is equivalent to learning
the conditional probability $P(C | X)$ for arbitrary $C$, $X$.

What can be approximated:
\begin{align}
  P(C_k) &\\
  P(X_i | C_k) &\\
  P(X_1, X_2, .. X_n | C_k) &= \Pi^n_{i =1} P(X_i | C_k)  
\end{align}

\begin{align}
  \chi &= X_1, X_2, X_3 ... X_n \\
  P(C_k | \chi) &= P (C_k | X_1, X_2, ... X_n) \\
  &= \frac{P(X_1, X_2, ... X_n | C_k) \cdot P(C)}{P(X_1, X_2, ... X_n)}
\end{align}



\end{document}